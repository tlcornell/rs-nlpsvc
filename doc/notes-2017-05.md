# 2017-05-03

## Tokenizer I/O Requirements

Tokenizer started out expecting a string to tokenize. 
Now I need to make it take a string and act on an AnnotatedDocument.
But (a) the string comes from that document to begin with; and (b) it binds a potentially generally useful utility to a very local data structure.

Looks like some traits might be useful here. 
What do we actually require of our input and output?

The input just has to be a source of characters, I think. 
Though someday it should become a source of bytes with character offsets, I think.

> NOTE (for future reference): A big problem with byte matching is what to do 
  with character classes 
  in the source regex. Translating character classes to byte-class automata
  is going to take some serious tooling.

The output is trickier, because realistically there isn't a large class of objects that respond to some sort of "add a token" method.
Still, if there's only one or two methods you need to define in this trait, then it at least becomes a lot easier to write wrappers for whatever sorts of things might be targets of tokenizer actions.

As an example, consider a `TokenSink` that just prints out token details (pretty much like what our sample actions do now). Calling `append` on such a sink would just call `println!` and be done.
The object in question has no actual state, just the `append` method.

Things get hairier if we want to support actions that rely on some kind of saved state. 
The body of the action function can't really know anything that isn't exposed in the generic trait, I think. I don't know if Rust supports any sort of *dynamic cast* that would be safe.

Maybe the problem is that we embed actions in the `ThompsonInterpreter` class.
Maybe instead it should just *emit the rule number*, and let someone else decide what to do with it.
More particularly, we are transducing a *stream of characters* into a *stream of MatchRecord* objects, which include both rule number and match size. 

* I think we have to add *match start* since the client won't be able to keep track of that. 
* Need to figure out what to do on match failure. Return some kind of NonMatchRecord?

Non-matches are always just one character long. We could distinguish them with a special rule number (for example, -1 or maybe 0). Or we could make `MatchRecord` just one variant of an *enum* that would include a `MatchFailure` variant as well.

> NOTE: An unmatched character may span multiple bytes of the input text,
  so when we get down to working at the byte level that is something we will need
  to be aware of.

So far, then: `ThompsonInterpreter::apply()` now needs a place to put its responses. 
So, roughly:

```
pub fn apply<T>(&mut self, text: &str, sink: &mut T)
	where T: TokenSink 
{
	let mut pos: usize = 0;
	while pos < text.len() {
		self.all_matches_at(&text[pos..]);
		match self.best_match() {
			None => {
				sink.append(MatchFailed {start: pos} );
				pos += 1;
			}
			Some(mtch) => {
				sink.append(mtch, &txt[pos..(pos+mtch.len)]);
				pos += mtch.len;
			}
		}
	}
}
```

Okay, that's two very different signatures for `TokenSink::append`.

Also, the client needs to be able to deal with `MatchRecord`, that is, they need to be able to see our definitions. Maybe break the struct up into multiple arguments, and define two methods instead of one. 

```
trait TokenSink {
	fn append_token(&mut self, begin: usize, len: usize, txt: &str, rule: usize);
	fn skip_char(&mut self, begin: usize, /*len: usize,*/ txt: &str /*char?*/);
}
```

Without different match variants, the function calls look a lot alike, but would be expected to behave very differently. Also, for the *skip* method:

* For now, `len` is always 1.
* And `txt` therefore might as well be `char`.

So now it is the `TokenSink` that is expected to implement the rules. 
How do we coordinate that with the `ThompsonInterpreter`?

* It's the `TokenizerBuilder` class that actually does the compiling of the rules.
* Internally, it adds rules to one list, and actions to another.
  So it can ensure that they both end up in the same order.

Basically, it is the TokenizerBuilder's job to build up a single program 
that then gets embedded in a ThompsonInterpreter. 
It builds it up rule by rule. 
It is the program that keeps track of the start addresses, 
which it updates each time `compile()` is called.

So a client, wishing to write a new tokenizer, will create a `TokenizerBuilder`, 
and call `add_rule` repeatedly.
They would no longer pass in a TokenizerAction reference. 
Instead, they would be creating some sort of list of actions, 
or map from rule numbers to actions, in their own code.
For instance, there could just be a `switch` statement somewhere, I guess.

They key step is that they then have to connect their own sink to the tokenizer
(presumably via some sort of `TokenizerBuilder::set_sink` method).
Something like the `switch` statement alluded to above would be part 
of the implementation of the `TokenSink::append` trait method.

So the main thing is that the TokenSink is something that will be called repeatedly.

### Annotating Documents

So an English tokenizer designed to work with an AnnotatedDocument object
would work by adding many rules for English tokens to the TokenizerBuilder, 
and attaching the AnnotatedDocument to its special TokenSink derivative.
Then it would get a reference to the text from the AnnotatedDocument, 
and feed that to the actual tokenizer built by the builder via the `run` method.

One thing: So far, I am attaching the *sink* to the builder, to be built into the tokenizer.
But I am passing in the *source* as an argument to the tokenizer.
It seems like the source and sink should be handled the same way.
So either both of them become part of the tokenizer object, or they are both arguments to `run`.

I think the latter is the better idea, for now.
If it turns out later that there may be different methods for creating a sink out of more basic parts, then maybe it should be the builder that knows about those, and therefore the source should also be created by the builder, so the tokenizer becomes a self contained blob, with input, program, and output all built in.
It represents a *tokenizer task*, in that case.

But that's a terrible idea, on the face of it. Because we don't want to go recompiling the program for every text we want to tokenize!
Better there should be some sort of `TokenizerTaskBuilder` in that case.


